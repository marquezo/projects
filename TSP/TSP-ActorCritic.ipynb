{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added critic to the code found in \n",
    "https://github.com/higgsfield/np-hard-deep-reinforcement-learning/blob/master/Neural%20Combinatorial%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a set of 2D coordinates of length 'num_samples'\n",
    "class TSPDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, num_nodes, num_samples, random_seed=111):\n",
    "        super(TSPDataset, self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        self.data_set = []\n",
    "        for l in tqdm(range(num_samples)):\n",
    "            x = torch.FloatTensor(2, num_nodes).uniform_(0, 1)\n",
    "            self.data_set.append(x)\n",
    "\n",
    "        self.size = len(self.data_set)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000000\n",
    "val_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:01<00:00, 510076.32it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 332501.27it/s]\n",
      "100%|██████████| 1000000/1000000 [00:02<00:00, 457638.65it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 317723.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_20_dataset = TSPDataset(20, train_size)\n",
    "val_20_dataset   = TSPDataset(20, val_size)\n",
    "\n",
    "train_50_dataset = TSPDataset(50, train_size)\n",
    "val_50_dataset   = TSPDataset(50, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 947.87it/s]\n"
     ]
    }
   ],
   "source": [
    "study_dataset = TSPDataset(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a vector of size equal to the mini-batch size\n",
    "def reward(sample_solution, USE_CUDA=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sample_solution list of length 'seq_len' of [batch_size x input_size (2 since doing 2D coordinates)]\n",
    "    \"\"\"\n",
    "    batch_size = sample_solution[0].size(0)\n",
    "    n = len(sample_solution)\n",
    "    #print batch_size\n",
    "    tour_len = Variable(torch.zeros([batch_size]))\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        tour_len = tour_len.cuda()\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        tour_len += torch.norm(sample_solution[i] - sample_solution[i + 1], dim=1)\n",
    "    \n",
    "    tour_len += torch.norm(sample_solution[n - 1] - sample_solution[0], dim=1)\n",
    "\n",
    "    return tour_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, use_tanh=False, C=10, name='Bahdanau', use_cuda=USE_CUDA):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.use_tanh = use_tanh\n",
    "        self.C = C\n",
    "        self.name = name\n",
    "        \n",
    "        if name == 'Bahdanau':\n",
    "            self.W_query = nn.Linear(hidden_size, hidden_size)\n",
    "            self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "\n",
    "            V = torch.FloatTensor(hidden_size)\n",
    "            if use_cuda:\n",
    "                V = V.cuda()  \n",
    "            self.V = nn.Parameter(V)\n",
    "            self.V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "            \n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: [batch_size x hidden_size]\n",
    "            ref:   ]batch_size x seq_len x hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = ref.size(0)\n",
    "        seq_len    = ref.size(1)\n",
    "        \n",
    "        if self.name == 'Bahdanau':\n",
    "            ref = ref.permute(0, 2, 1)\n",
    "            query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]\n",
    "            ref   = self.W_ref(ref)  # [batch_size x hidden_size x seq_len] \n",
    "            expanded_query = query.repeat(1, 1, seq_len) # [batch_size x hidden_size x seq_len]\n",
    "            V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1) # [batch_size x 1 x hidden_size]\n",
    "            logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)\n",
    "            \n",
    "        elif self.name == 'Dot':\n",
    "            query  = query.unsqueeze(2)\n",
    "            logits = torch.bmm(ref, query).squeeze(2) #[batch_size x seq_len x 1]\n",
    "            ref = ref.permute(0, 2, 1)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            logits = self.C * F.tanh(logits)\n",
    "        else:\n",
    "            logits = logits  \n",
    "        return ref, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, use_cuda=USE_CUDA):\n",
    "        super(GraphEmbedding, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.embedding = nn.Parameter(torch.FloatTensor(input_size, embedding_size)) \n",
    "        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(2)\n",
    "        embedding = self.embedding.repeat(batch_size, 1, 1)  \n",
    "        embedded = []\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        for i in range(seq_len):\n",
    "            embedded.append(torch.bmm(inputs[:, :, :, i].float(), embedding))\n",
    "        embedded = torch.cat(embedded, 1)\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            num_processing,\n",
    "            n_glimpses,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.num_processing = num_processing\n",
    "        self.use_cuda       = use_cuda\n",
    "        \n",
    "        self.embedding = GraphEmbedding(2, embedding_size, use_cuda=use_cuda)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        #self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)  \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        #TODO change to Dima's and take care of dimensions of ref (i.e., permutation)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=True, name='Bahdanau', use_cuda=use_cuda) #Bahdanau, use_tanh=True           \n",
    "            \n",
    "        self.fc1.weight = torch.nn.init.uniform(self.fc1.weight, -0.08, 0.08)  \n",
    "        self.fc2.weight = torch.nn.init.uniform(self.fc2.weight, -0.08, 0.08)\n",
    "            \n",
    "            \n",
    "    def forward(self, inputs, input_embedded):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x 1 x sourceL]\n",
    "        \"\"\"        \n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(2)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        #embedded = self.embedding(inputs)\n",
    "        \n",
    "        #The encoder simply runs the embedding\n",
    "        encoder_outputs, (hidden, context) = self.encoder(input_embedded)       \n",
    "        \"\"\"encoder_outputs: [batch_size x seq_len x hidden_size]\"\"\"                \n",
    "       \n",
    "        #The first input to the decoder is the last hidden state\n",
    "        #decoder_input = torch.t(hidden) #Batch size has to be the first dimension, so swap first and second dimensions\n",
    "        \n",
    "        #Init decoder's hidden and reuse the encoder's context        \n",
    "        #hidden = Variable(torch.zeros(1, batch_size, hidden_size))\n",
    "        #context = Variable(torch.zeros(context.size()))\n",
    "        \n",
    "#         print (decoder_input.size())\n",
    "        \n",
    "        query = torch.t(hidden).squeeze()        \n",
    "        \n",
    "        #For each step in the sequence\n",
    "        for i in range(self.num_processing):                        \n",
    "            \n",
    "            #_, (hidden, context) = self.decoder(decoder_input, (hidden, context))\n",
    "            \n",
    "            #query = hidden.squeeze(0) #[hidden_size x 1] (or the other way around)            \n",
    "            \n",
    "            #Do the glimpse\n",
    "            ref, logits = self.glimpse(query, encoder_outputs)\n",
    "            #logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)           \n",
    "                    \n",
    "            #[batch_size x hidden_size]\n",
    "            decoder_input = query#.unsqueeze(1)\n",
    "            \n",
    "        #Do fully connected part   TODO: batch norm \n",
    "        output = self.fc1(query)\n",
    "        output = F.relu(output)\n",
    "        output = self.fc2(output)\n",
    "            \n",
    "        #list of seq_len containing[batch_size x seq_len], list of seq_len containing [batch_size]    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNet(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            attention,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNet, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "        \n",
    "        \n",
    "        self.embedding = GraphEmbedding(2, embedding_size, use_cuda=use_cuda)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, name=attention, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, name=attention, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "    \"\"\"\n",
    "    idxs: indeces that were previously chosen\n",
    "    logits: probabilities for current step\n",
    "    \"\"\"\n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x 1 x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(2)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        \n",
    "        #The encoder simply runs the embedding\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)        \n",
    "        \n",
    "        prev_probs = []\n",
    "        prev_idxs = []\n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        #The first input to the decoder is learned, as in the paper\n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        #For each step in the sequence\n",
    "        for i in range(seq_len):                                        \n",
    "                \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0) #[hidden_size x 1] (or the other way around)\n",
    "            \n",
    "            for i in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)                 \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            \n",
    "            #[batch size x seq_len]\n",
    "            probs = F.softmax(logits)\n",
    "            \n",
    "            #Give me the index that will be chosen: [batch_size]\n",
    "            idxs = probs.multinomial().squeeze(1)\n",
    "            \n",
    "            for old_idxs in prev_idxs:\n",
    "                if old_idxs.eq(idxs).data.any():\n",
    "                    print seq_len\n",
    "                    print(' RESAMPLE!')\n",
    "                    idxs = probs.multinomial().squeeze(1)\n",
    "                    break\n",
    "                    \n",
    "            #[batch_size x hidden_size]\n",
    "            decoder_input = embedded[range(batch_size), idxs.data, :] \n",
    "            \n",
    "            prev_probs.append(probs)\n",
    "            prev_idxs.append(idxs)\n",
    "            \n",
    "        #list of seq_len containing[batch_size x seq_len], list of seq_len containing [batch_size]    \n",
    "        return prev_probs, prev_idxs, embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinatorialRL(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            reward,\n",
    "            attention,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(CombinatorialRL, self).__init__()\n",
    "        self.reward = reward\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.actor = PointerNet(\n",
    "                embedding_size,\n",
    "                hidden_size,\n",
    "                seq_len,\n",
    "                n_glimpses,\n",
    "                tanh_exploration,\n",
    "                use_tanh,\n",
    "                attention,\n",
    "                use_cuda)\n",
    "        \n",
    "        self.critic = Critic(embedding_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            seq_len=seq_len,\n",
    "            num_processing=3,\n",
    "            n_glimpses=n_glimpses,\n",
    "            use_cuda=use_cuda)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: [batch_size, input_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        input_size = inputs.size(1) #2 because we are doing 2D coordinates\n",
    "        seq_len    = inputs.size(2)        \n",
    "                \n",
    "        #list of seq_len containing[batch_size x seq_len], list of seq_len containing [batch_size]    \n",
    "        probs, action_idxs, input_embedded = self.actor(inputs)\n",
    "                        \n",
    "        critic_evals = self.critic(inputs, input_embedded)\n",
    "        \n",
    "#         print (\"Combinatorial RL (inputs)\")\n",
    "#         print (inputs)\n",
    "        #print (\"Combinatorial RL (action_idxs.size): \", action_idxs)\n",
    "       \n",
    "        actions = []\n",
    "        \n",
    "        \"\"\"\n",
    "        Transpose the inputs to have [batch_size, seq_len, input_size]\n",
    "        \"\"\"\n",
    "        inputs = inputs.transpose(1, 2)\n",
    "        \n",
    "        #List of size seq_len\n",
    "        for action_id in action_idxs:\n",
    "            actions.append(inputs[range(batch_size), action_id.data, :])\n",
    "            \n",
    "        #actions now has the coordinates in the solution\n",
    "            \n",
    "        action_probs = []    \n",
    "        #List of size seq_len\n",
    "        for prob, action_id in zip(probs, action_idxs):\n",
    "            #We want to know the probability of taking each action (picking city) in the solution\n",
    "            action_probs.append(prob[range(batch_size), action_id.data])\n",
    "\n",
    "        #R is [batch_size x 1]\n",
    "        R = self.reward(actions, self.use_cuda)\n",
    "        #print (\"Total reward: \", R)\n",
    "        \n",
    "        return R, action_probs, actions, action_idxs, critic_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size    = 128\n",
    "n_glimpses = 1\n",
    "tanh_exploration = 10\n",
    "use_tanh = True\n",
    "\n",
    "beta = 0.9\n",
    "max_grad_norm = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp_20_model = CombinatorialRL(\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        20,\n",
    "        n_glimpses, \n",
    "        tanh_exploration,\n",
    "        use_tanh,\n",
    "        reward,\n",
    "        attention=\"Dot\",\n",
    "        use_cuda=USE_CUDA)\n",
    "\n",
    "tsp_50_model = CombinatorialRL(\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        50,\n",
    "        n_glimpses, \n",
    "        tanh_exploration,\n",
    "        use_tanh,\n",
    "        reward,\n",
    "        attention=\"Bahdanau\",\n",
    "        use_cuda=USE_CUDA)\n",
    "\n",
    "tsp_5_model = CombinatorialRL(\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        5,\n",
    "        n_glimpses,\n",
    "        tanh_exploration,\n",
    "        use_tanh,\n",
    "        reward,\n",
    "        attention='Dot',\n",
    "        use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "    def __init__(self, model, train_dataset, val_dataset, batch_size=128, threshold=None, max_grad_norm=2.):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset   = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "        self.val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        self.actor_optim   = optim.Adam(model.actor.parameters(), lr=0.0001)\n",
    "        self.critic_optim  = optim.Adam(model.critic.parameters(), lr=0.0001)\n",
    "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.actor_optim, step_size=5000, gamma=0.96)\n",
    "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.critic_optim, step_size=5000, gamma=0.96)\n",
    "        \n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.train_tour = []\n",
    "        self.val_tour   = []\n",
    "        \n",
    "        self.epochs = 0\n",
    "    \n",
    "    def train_and_validate(self, n_epochs):\n",
    "        critic_exp_mvg_avg = torch.zeros(1)\n",
    "        critic_loss_criterion = torch.nn.MSELoss()        \n",
    "        \n",
    "        if USE_CUDA: \n",
    "            critic_exp_mvg_avg = critic_exp_mvg_avg.cuda()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_id, sample_batch in enumerate(self.train_loader):\n",
    "                self.model.train()\n",
    "                \n",
    "                self.scheduler_actor.step()\n",
    "                self.scheduler_critic.step()\n",
    "                \n",
    "                inputs = Variable(sample_batch)\n",
    "                \n",
    "                if USE_CUDA:\n",
    "                    inputs = inputs.cuda()\n",
    "\n",
    "                #Model is combinatorial\n",
    "                R, probs, actions, actions_idxs, values = self.model(inputs)\n",
    "\n",
    "                if batch_id == 0:\n",
    "                    critic_exp_mvg_avg = R.mean()\n",
    "                else:\n",
    "                    critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())\n",
    "\n",
    "                #Vector of length equal to the mini-batch size: Q(s,a) - V(s)\n",
    "                #advantage = R - critic_exp_mvg_avg\n",
    "                \n",
    "                advantage = R.unsqueeze(1) - values\n",
    "                \n",
    "                print (\"Advantage function: \", R.mean() - values.mean())\n",
    "\n",
    "                logprobs = 0\n",
    "                for prob in probs: \n",
    "                    logprob = torch.log(prob)\n",
    "                    logprobs += logprob\n",
    "                    \n",
    "                #logprobs[logprobs < -1000] = 0. #Works with PyTorch 2.0\n",
    "                                \n",
    "                #For Pytorch 3.0                                \n",
    "                if logprobs.data[0] < -1000:\n",
    "                    print (logprobs.data[0])\n",
    "                    logprobs = Variable(torch.FloatTensor([0.]), requires_grad=True)\n",
    "\n",
    "                reinforce = advantage * logprobs\n",
    "                actor_loss = reinforce.mean()\n",
    "\n",
    "                self.actor_optim.zero_grad()                \n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                torch.nn.utils.clip_grad_norm(self.model.actor.parameters(),\n",
    "                                    float(self.max_grad_norm), norm_type=2)\n",
    "\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "                #Do critic gradient descent\n",
    "                self.critic_optim.zero_grad()\n",
    "                loss_critic = critic_loss_criterion(values, R.unsqueeze(1))\n",
    "                loss_critic.backward()\n",
    "                torch.nn.utils.clip_grad_norm(self.model.critic.parameters(),\n",
    "                    float(self.max_grad_norm), norm_type=2)\n",
    "\n",
    "                self.critic_optim.step()\n",
    "                #print (\"Critic's loss: \", loss_critic.data[0])\n",
    "                \n",
    "\n",
    "                #critic_exp_mvg_avg = critic_exp_mvg_avg.detach()\n",
    "\n",
    "                self.train_tour.append(R.mean().data[0])\n",
    "\n",
    "                if batch_id % 10 == 0:\n",
    "                    self.plot(self.epochs)\n",
    "\n",
    "#                 if batch_id % 100 == 0:    \n",
    "\n",
    "#                     self.model.eval()\n",
    "#                     for val_batch in self.val_loader:\n",
    "#                         inputs = Variable(val_batch)\n",
    "                        \n",
    "#                         if USE_CUDA:\n",
    "#                             inputs = inputs.cuda()\n",
    "\n",
    "#                         R, probs, actions, actions_idxs, values = self.model(inputs)\n",
    "#                         self.val_tour.append(R.mean().data[0])\n",
    "\n",
    "            if self.threshold and self.train_tour[-1] < self.threshold:\n",
    "                print \"EARLY STOPPAGE!\"\n",
    "                break\n",
    "                \n",
    "            self.epochs += 1\n",
    "                \n",
    "    def plot(self, epoch):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('train tour length: epoch %s reward %s' % (epoch, self.train_tour[-1] if len(self.train_tour) else 'collecting'))\n",
    "        plt.plot(self.train_tour)\n",
    "        plt.grid()\n",
    "#         plt.subplot(132)\n",
    "#         plt.title('val tour length: epoch %s reward %s' % (epoch, self.val_tour[-1] if len(self.val_tour) else 'collecting'))\n",
    "#         plt.plot(self.val_tour)\n",
    "#         plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp_20_train = TrainModel(tsp_20_model, \n",
    "                        train_20_dataset, \n",
    "                        val_20_dataset, \n",
    "                        threshold=3.99)\n",
    "\n",
    "tsp_5_train = TrainModel(tsp_5_model, study_dataset,\n",
    "                        study_dataset, threshold=3.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsp_20_train.train_and_validate(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
