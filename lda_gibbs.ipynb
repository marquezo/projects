{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "#\n",
    "# Train data must be of the form\n",
    "# <document_index> <word_index in the vocabulary> <token count>\n",
    "#\n",
    "# where document_index and word_index start at 1\n",
    "#################################################################################################################\n",
    "class LDA_gibbs_sampler:\n",
    "    \n",
    "    def __init__(self, num_topics, alpha=1, eta=0.1, num_iterations=200, save_samples_sched = None):\n",
    "        self.K = num_topics\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "        self.num_iterations=num_iterations\n",
    "        self.count_vectorizer = CountVectorizer(stop_words='english', min_df=2, max_df=0.95)\n",
    "        self.save_samples_sched = save_samples_sched\n",
    "        \n",
    "    #Return log likelihood for p(w|z) and p(z)\n",
    "    def compute_log_likelihood(self, C_wt, C_dt):\n",
    "        subtractor_w = gammaln(C_wt.sum(axis=0) + len(self.vocab)*self.eta).sum() # for log p(w|z)\n",
    "        subtractor_d = gammaln(C_dt.sum(axis=1) + self.K*self.alpha).sum() # for log p(z)\n",
    "    \n",
    "        return gammaln(C_wt + self.eta).sum() - subtractor_w, gammaln(C_dt + self.alpha).sum() - subtractor_d            \n",
    "       \n",
    "    #Return count matrices\n",
    "    # C_wt (i,j): number of times word i was assigned to topic j\n",
    "    # C_dt (i,j): number of times a word in doc i was assigned to topic j\n",
    "    def calculate_counts(self, sample):\n",
    "        C_wt_z = np.zeros((len(self.vocab), self.K))\n",
    "        C_dt_z = np.zeros((self.num_docs, self.K))\n",
    "\n",
    "        #Populate counts given z\n",
    "        for i in range(sample.shape[0]): #number of docs\n",
    "            for j in range(sample.shape[1]): #number of words in vocab\n",
    "                C_wt_z[j, sample[i,j]]+=1\n",
    "                C_dt_z[i, sample[i,j]]+=1\n",
    "                \n",
    "        return C_wt_z, C_dt_z        \n",
    "        \n",
    "    def train(self, traindata, num_docs, vocab, verbose=False):\n",
    "                       \n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.num_docs = num_docs\n",
    "        size_vocab = len(self.vocab)\n",
    "\n",
    "        #Per G&S(2004)\n",
    "        self.C_wt = np.zeros((size_vocab, self.K))\n",
    "        self.C_dt = np.zeros((self.num_docs, self.K))\n",
    "\n",
    "        #Topic-word assignments: Z[i,j] = k => topic k is responsible for word token j in doc i\n",
    "        #Random init\n",
    "        self.Z = np.random.randint(self.K, size=(num_docs, size_vocab))\n",
    "        \n",
    "        if (self.save_samples_sched is not None):\n",
    "            self.samples = np.zeros((len(self.save_samples_sched), num_docs, size_vocab))\n",
    "            sample_idx = 0\n",
    "    \n",
    "        for doc_idx, word_idx, token_count in traindata.astype(int):\n",
    "            #the bag of words file starts at 1\n",
    "            doc_idx-=1\n",
    "            word_idx-=1\n",
    "\n",
    "            topic_assignment = self.Z[doc_idx, word_idx]\n",
    "            self.C_wt[word_idx, topic_assignment]+=token_count\n",
    "            self.C_dt[doc_idx, topic_assignment]+=token_count          \n",
    "        \n",
    "        iteration = 0\n",
    "        \n",
    "        size_vocab_times_eta = (size_vocab*self.eta)\n",
    "        K_times_alpha = (self.K*self.alpha)\n",
    "        p = np.zeros(self.K)\n",
    "\n",
    "        while iteration < self.num_iterations:            \n",
    "                        \n",
    "            if (verbose):\n",
    "                print np.sum(self.compute_log_likelihood(self.C_wt, self.C_dt))\n",
    "                \n",
    "            if (iteration%10 == 0 and verbose):\n",
    "                print iteration                           \n",
    "\n",
    "            for doc_idx, word_idx, token_count in traindata.astype(int):\n",
    "                #the bag of words file starts at 1\n",
    "                doc_idx-=1\n",
    "                word_idx-=1\n",
    "\n",
    "                topic_assignment = self.Z[doc_idx, word_idx]\n",
    "                #For each word token,the count matrices CWT and CDT are first decremented by one for the \n",
    "                #entries that correspond to the current topic assignment\n",
    "                self.C_wt[word_idx, topic_assignment]-=token_count\n",
    "                self.C_dt[doc_idx, topic_assignment]-=token_count\n",
    "                #Then, a new topic is sampled from the distribution in Equation 3 and the count matrices CWT and CDT\n",
    "                #are incremented with the new topic assignment                \n",
    "                \n",
    "                sum_dt_all_topics = np.sum(self.C_dt[doc_idx, :])\n",
    "\n",
    "                for k in range(self.K):      \n",
    "                    denominator = (np.sum(self.C_wt[:,k]) + size_vocab_times_eta)*(sum_dt_all_topics + K_times_alpha)\n",
    "                    p[k] = (self.C_wt[word_idx, k] + self.eta)*(self.C_dt[doc_idx, k] + self.alpha)/denominator                  \n",
    "                    \n",
    "                p = p/np.sum(p)   \n",
    "\n",
    "                #argmax will return the topic selected\n",
    "                new_topic = np.argmax(np.random.multinomial(1, p))\n",
    "\n",
    "                self.Z[doc_idx, word_idx] = new_topic\n",
    "                #and the count matrices CWT and CDT are incremented with the new topic assignment\n",
    "                self.C_wt[word_idx, new_topic]+=token_count\n",
    "                self.C_dt[doc_idx, new_topic]+=token_count\n",
    "\n",
    "            iteration+=1\n",
    "            \n",
    "            if (self.save_samples_sched is not None and iteration in self.save_samples_sched):\n",
    "                self.samples[sample_idx] = self.Z\n",
    "                sample_idx+=1                                \n",
    "\n",
    "    #C_dt: matrix (num_docs x num_topics) where C_dt(d, j)=number of times a word from doc d has been assigned to topic j\n",
    "    def compute_theta(self, C_dt):\n",
    "        #Infer distribution of topics per document\n",
    "        to_return = np.zeros(C_dt.shape)\n",
    "\n",
    "        for d in range(C_dt.shape[0]):\n",
    "            to_return[d]= [ (C_dt[d, j] + self.alpha)/(np.sum(C_dt[d, :]) + (self.K*self.alpha)) for j in range(self.K) ]\n",
    "\n",
    "        return to_return\n",
    "            \n",
    "    #C_wt: matrix (size_vocab x num_topics) where C_wt(w, j)=number of times word w has been assigned to topic j        \n",
    "    def compute_beta(self, C_wt):\n",
    "        #Infer distribution of words per topic\n",
    "        size_vocab = C_wt.shape[0]\n",
    "        to_return = np.zeros((self.K, size_vocab))    \n",
    "\n",
    "        for k in range(self.K):\n",
    "            to_return[k]= [ (C_wt[w,k] + self.eta)/(np.sum(C_wt[:, k]) + (size_vocab*self.eta)) for w in range(size_vocab)]\n",
    "    \n",
    "        return to_return\n",
    "    \n",
    "    def print_topic_words(self, num_words, beta):\n",
    "        feature_names = self.vocab\n",
    "        \n",
    "        for topic_id, topic in enumerate(beta):\n",
    "            sorted_topic = topic.argsort()[:-num_words-1:-1]\n",
    "            print \"Topic %d: \" % topic_id  + \", \".join([feature_names[i] for i in sorted_topic]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: basketball, sports, soccer, hockey, like\n",
      "Topic 1: windy, cold, dry, weather, freezing\n"
     ]
    }
   ],
   "source": [
    "#Since our gibbs sampler will iterate through all word tokens in the corpus,\n",
    "#test that after using scikit-learn's tokenizer, we are passing the data as expected\n",
    "data_samples = [\n",
    "    'weather: warm, cold, freezing, hot, windy, windy',\n",
    "    'weather: dry, windy, moist, cold, etc',\n",
    "    'freezing means dry and windy',\n",
    "    'sports game, be it basketball, hockey or soccer, I feel better',\n",
    "    'sports can be soothing. hockey, but I like soccer and basketball'\n",
    "]\n",
    "\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "# for i in range(len(data_samples)):\n",
    "#     print \"Document {}\".format(i)\n",
    "#     print tf.data[tf.indptr[i]:tf.indptr[i+1]]\n",
    "#     print tf.indices[tf.indptr[i]:tf.indptr[i+1]]\n",
    "\n",
    "dummy_data =  np.ndarray(shape=(tf.data.shape[0], 3), dtype=int)\n",
    "doc_idx = 0\n",
    "    \n",
    "# print \"Doc idx | word idx | token count\"\n",
    "for idx, token_count in enumerate(tf.data):\n",
    "    if idx >= tf.indptr[doc_idx+1]:\n",
    "        doc_idx+=1\n",
    "    \n",
    "    word_idx = tf.indices[idx]\n",
    "    dummy_data[idx] = np.array([doc_idx+1, word_idx+1, token_count])\n",
    "\n",
    "#Test on dummy data\n",
    "keylist = tf_vectorizer.vocabulary_.keys()\n",
    "keylist.sort()\n",
    "    \n",
    "lda_gibbs_sampler = LDA_gibbs_sampler(2, alpha=1, eta=0.01, num_iterations=20000)\n",
    "lda_gibbs_sampler.train(traindata=dummy_data, num_docs=5, vocab=keylist)\n",
    "theta = lda_gibbs_sampler.compute_theta(lda_gibbs_sampler.C_dt)\n",
    "beta = lda_gibbs_sampler.compute_beta(lda_gibbs_sampler.C_wt)\n",
    "lda_gibbs_sampler.print_topic_words(5, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run on the KOS blog data set\n",
    "text_file = open('vocab.kos.txt', \"r\")\n",
    "vocab = text_file.read().splitlines()\n",
    "kos_data=np.loadtxt('docword.kos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the num_iterations parameter to run for at least 1000 iterations\n",
    "lda_gibbs_sampler = LDA_gibbs_sampler(10, alpha=5, eta=0.1, num_iterations=10)\n",
    "lda_gibbs_sampler.train(traindata=kos_data, num_docs=3430, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To run on the 20newsgroups dataset\n",
    "dataset_20ng = fetch_20newsgroups(subset='train', shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "num_docs_to_load = 20\n",
    "\n",
    "tf_20ng = tf_vectorizer.fit_transform(dataset_20ng.data[:num_docs_to_load])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
